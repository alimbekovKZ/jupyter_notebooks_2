{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/didi/anaconda3/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning:\n",
      "\n",
      "Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.1) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from hyperopt import fmin, hp, tpe, space_eval\n",
    "\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit\n",
    "import lightgbm as lgb\n",
    "from time import time\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import os\n",
    "\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../temp/train_temp2000.csv')\n",
    "test = pd.read_csv('../temp/test_temp2000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 434)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 434)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sample(frac=0.9, replace=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'num_leaves': 491,\n",
    "          'min_child_weight': 0.03454472573214212,\n",
    "          'feature_fraction': 0.3797454081646243,\n",
    "          'bagging_fraction': 0.4181193142567742,\n",
    "          'min_data_in_leaf': 106,\n",
    "          'objective': 'binary',\n",
    "          'max_depth': -1,\n",
    "          'learning_rate': 0.006883242363721497,\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"bagging_seed\": 11,\n",
    "          \"metric\": 'auc',\n",
    "          \"verbosity\": -1,\n",
    "          'reg_alpha': 0.3899927210061127,\n",
    "          'reg_lambda': 0.6485237330340494,\n",
    "          'random_state': 47\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "aucs = list()\n",
    "feature_importances = pd.DataFrame()\n",
    "feature_importances['feature'] = X.columns\n",
    "\n",
    "training_start_time = time()\n",
    "for fold, (trn_idx, test_idx) in enumerate(folds.split(X, y)):\n",
    "    start_time = time()\n",
    "    print('Training on fold {}'.format(fold + 1))\n",
    "    \n",
    "    trn_data = lgb.Dataset(X.iloc[trn_idx], label=y.iloc[trn_idx])\n",
    "    val_data = lgb.Dataset(X.iloc[test_idx], label=y.iloc[test_idx])\n",
    "#     clf = lgb.train(params, trn_data, num_boost_round = 10000, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds=500)\n",
    "    clf = lgb.train(params, trn_data, num_boost_round = 10000, valid_sets = [val_data], verbose_eval=100, early_stopping_rounds=500)\n",
    "    \n",
    "    feature_importances['fold_{}'.format(fold + 1)] = clf.feature_importance()\n",
    "    aucs.append(clf.best_score['valid_0']['auc'])\n",
    "    \n",
    "    print('Fold {} finished in {}'.format(fold + 1, str(datetime.timedelta(seconds=time() - start_time))))\n",
    "print('-' * 30)\n",
    "print('Training has finished.')\n",
    "print('Total training time is {}'.format(str(datetime.timedelta(seconds=time() - training_start_time))))\n",
    "print('Mean AUC:', np.mean(aucs))\n",
    "print('-' * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_bagging(train, train_y, test):\n",
    "    \n",
    "#     随机采样数据\n",
    "    \n",
    "#     随机生成参数\n",
    "\n",
    "#     训练模型\n",
    "\n",
    "#     保存结果\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf right now is the last model, trained with 80% of data and validated with 20%\n",
    "best_iter = clf.best_iteration\n",
    "print(\"best_iteration: \", best_iter)\n",
    "# clf = lgb.LGBMClassifier(**params, num_boost_round=int(best_iter * 1.1))\n",
    "# clf.fit(X, y)\n",
    "\n",
    "all_data = lgb.Dataset(X, label=y)\n",
    "all_clf  = lgb.train(params, all_data, num_boost_round = int(best_iter * 1.20), valid_sets = [all_data], verbose_eval=100) \n",
    "\n",
    "# sub['isFraud'] = clf.predict_proba(test_X)[:, 1]\n",
    "sub['isFraud'] = all_clf.predict(test_X)\n",
    "sub.to_csv('ieee_cis_fraud_detection_v6.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
