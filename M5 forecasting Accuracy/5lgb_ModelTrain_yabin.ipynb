{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lgb model \n",
    "- <font color=red> Check\n",
    "    - <font color=red> store id \n",
    "    - <font color=red> features\n",
    "    - <font color=red> version\n",
    "    - <font color=red> original file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, time, warnings, pickle, psutil, random, time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from math import ceil\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the file path if run on different machines\n",
    "MainFilePath = 'MainData/'\n",
    "ModelFilePath = 'Model/'\n",
    "PredictFilePath = 'Predict/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "def get_data_by_store(store):\n",
    "    # Read and contact basic feature\n",
    "    df = pd.concat([pd.read_pickle(MainFilePath+'grid_part123.pkl'), \n",
    "                     pd.read_pickle(MainFilePath+'lag_rolling.pkl').iloc[:,3:],\n",
    "                     pd.read_pickle(MainFilePath+'encoding.pkl').iloc[:,10:]], axis=1)\n",
    "    \n",
    "    # Leave only relevant store\n",
    "    df = df[df['store_id']==store]\n",
    "          \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set global vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "VER = 5 #model version\n",
    "SEED = 1            \n",
    "seed_everything(SEED)            \n",
    "\n",
    "USE_AUX = True               # Use or not pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'sales'\n",
    "START_TRAIN = 0\n",
    "#END_TRAIN = 1913\n",
    "END_TRAIN = 1941\n",
    "P_HORIZON = 28 \n",
    "STORES_IDS = ['CA_1','CA_2','CA_3','CA_4','TX_1','TX_2','TX_3','WI_1','WI_2','WI_3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train on all data before 1913\n",
    "- The \"fake\" validation set is the last 28 days of 1913\n",
    "- The test data is 1913 to 1941"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red> **Attention to feature selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Customized_features = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'd',\n",
    "       'sales', 'release', 'wm_yr_wk', 'enc_state_id_mean', 'enc_state_id_std',\n",
    "       'enc_store_id_mean', 'enc_store_id_std', 'enc_cat_id_mean',\n",
    "       'enc_cat_id_std', 'enc_dept_id_mean', 'enc_dept_id_std',\n",
    "       'enc_state_id_cat_id_mean', 'enc_state_id_cat_id_std',\n",
    "       'enc_state_id_dept_id_mean', 'enc_state_id_dept_id_std',\n",
    "       'enc_store_id_cat_id_mean', 'enc_store_id_cat_id_std',\n",
    "       'enc_store_id_dept_id_mean', 'enc_store_id_dept_id_std',\n",
    "       'enc_item_id_mean', 'enc_item_id_std', 'enc_item_id_state_id_mean',\n",
    "       'enc_item_id_state_id_std', 'enc_item_id_store_id_mean',\n",
    "       'enc_item_id_store_id_std']\n",
    "\n",
    "Basic_features = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'd',\n",
    "       'sales', 'release', 'wm_yr_wk', 'sell_price', 'price_max', 'price_min',\n",
    "       'price_std', 'price_mean', 'price_norm', 'price_nunique',\n",
    "       'item_nunique', 'price_momentum_m', 'price_momentum_y',\n",
    "       'price_momentum', 'event_name_1', 'event_type_1', 'event_name_2',\n",
    "       'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'tm_d', 'tm_w', 'tm_m',\n",
    "       'tm_y', 'tm_dw', 'tm_wm', 'tm_w_end']\n",
    "\n",
    "# Lag_features are important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# according to Konstantin Yakovlev's brilliant idea, some featues will be removed\n",
    "remove_features = [TARGET,'id', 'item_id', 'dept_id', 'cat_id','state_id','store_id','date',\n",
    "                   'wm_yr_wk','d','price_nunique','item_nunique', 'rolling_mean7',\n",
    "                   'rolling_std7', 'rolling_mean28', 'rolling_std28','enc_state_id_mean', 'enc_state_id_std',\n",
    "                   'enc_cat_id_mean', 'enc_cat_id_std', 'enc_dept_id_mean', 'enc_dept_id_std','enc_state_id_cat_id_mean', \n",
    "                   'enc_state_id_cat_id_std', 'enc_state_id_dept_id_mean', 'enc_state_id_dept_id_std','enc_item_id_mean', \n",
    "                   'enc_item_id_std', 'enc_item_id_state_id_mean', 'enc_item_id_state_id_std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "                    'boosting_type': 'gbdt',         # Standard boosting type\n",
    "                    'objective': 'tweedie',          # poisson, tweedie\n",
    "                    'metric': ['rmse'],              # as we will use rmse as metric \"proxy\"\n",
    "                    'tweedie_variance_power': 1.1,\n",
    "                    'subsample': 0.5,                ### 0.3, 0.8\n",
    "                    'subsample_freq': 1,             ### \n",
    "                    'learning_rate': 0.075,           ### 0.5 is \"fast enough\" for us\n",
    "                    'num_leaves': 2**11-1,           # We will need model only for fast check\n",
    "                    'min_data_in_leaf': 2**12-1,     # So we want it to train faster even with drop in generalization \n",
    "                    'feature_fraction': 0.5,         ###\n",
    "                    'n_estimators': 1500,            # We don't want to limit training (you can change 5000 to any big enough number)\n",
    "                    'max_bin':100,\n",
    "                    'boost_from_average':False,\n",
    "                    'early_stopping_rounds': 30,     # We will stop training almost immediately (if it stops improving) \n",
    "                    'seed': SEED,\n",
    "                    'verbose': -1,\n",
    "                } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CA_1 ...\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's rmse: 2.01953\n",
      "[200]\tvalid_0's rmse: 2.00714\n",
      "[300]\tvalid_0's rmse: 1.99507\n",
      "[400]\tvalid_0's rmse: 1.98447\n",
      "[500]\tvalid_0's rmse: 1.97546\n",
      "[600]\tvalid_0's rmse: 1.9669\n",
      "[700]\tvalid_0's rmse: 1.96078\n",
      "[800]\tvalid_0's rmse: 1.95455\n",
      "[900]\tvalid_0's rmse: 1.9483\n",
      "[1000]\tvalid_0's rmse: 1.94101\n",
      "[1100]\tvalid_0's rmse: 1.935\n",
      "[1200]\tvalid_0's rmse: 1.93014\n",
      "[1300]\tvalid_0's rmse: 1.92448\n",
      "[1400]\tvalid_0's rmse: 1.91884\n",
      "[1500]\tvalid_0's rmse: 1.91277\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1500]\tvalid_0's rmse: 1.91277\n",
      "Training CA_2 ...\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's rmse: 1.91214\n",
      "[200]\tvalid_0's rmse: 1.88881\n",
      "[300]\tvalid_0's rmse: 1.87615\n",
      "[400]\tvalid_0's rmse: 1.86611\n",
      "[500]\tvalid_0's rmse: 1.85678\n",
      "[600]\tvalid_0's rmse: 1.84865\n",
      "[700]\tvalid_0's rmse: 1.84066\n",
      "[800]\tvalid_0's rmse: 1.83446\n",
      "[900]\tvalid_0's rmse: 1.82839\n",
      "[1000]\tvalid_0's rmse: 1.82181\n",
      "[1100]\tvalid_0's rmse: 1.81524\n",
      "[1200]\tvalid_0's rmse: 1.81031\n",
      "[1300]\tvalid_0's rmse: 1.80513\n",
      "[1400]\tvalid_0's rmse: 1.79949\n",
      "[1500]\tvalid_0's rmse: 1.79465\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1499]\tvalid_0's rmse: 1.7946\n",
      "Training CA_3 ...\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's rmse: 2.38891\n",
      "[200]\tvalid_0's rmse: 2.36332\n",
      "[300]\tvalid_0's rmse: 2.34538\n",
      "[400]\tvalid_0's rmse: 2.33045\n",
      "[500]\tvalid_0's rmse: 2.31695\n",
      "[600]\tvalid_0's rmse: 2.30828\n",
      "[700]\tvalid_0's rmse: 2.29837\n",
      "[800]\tvalid_0's rmse: 2.28767\n",
      "[900]\tvalid_0's rmse: 2.27668\n",
      "[1000]\tvalid_0's rmse: 2.26797\n",
      "[1100]\tvalid_0's rmse: 2.26228\n",
      "[1200]\tvalid_0's rmse: 2.25461\n",
      "[1300]\tvalid_0's rmse: 2.24733\n",
      "[1400]\tvalid_0's rmse: 2.24064\n",
      "[1500]\tvalid_0's rmse: 2.23559\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1500]\tvalid_0's rmse: 2.23559\n",
      "Training CA_4 ...\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's rmse: 1.39324\n",
      "[200]\tvalid_0's rmse: 1.38548\n",
      "[300]\tvalid_0's rmse: 1.37882\n",
      "[400]\tvalid_0's rmse: 1.37333\n",
      "[500]\tvalid_0's rmse: 1.36812\n",
      "[600]\tvalid_0's rmse: 1.36353\n",
      "[700]\tvalid_0's rmse: 1.35917\n",
      "[800]\tvalid_0's rmse: 1.35544\n",
      "[900]\tvalid_0's rmse: 1.35102\n",
      "[1000]\tvalid_0's rmse: 1.34718\n",
      "[1100]\tvalid_0's rmse: 1.34388\n",
      "[1200]\tvalid_0's rmse: 1.33945\n",
      "[1300]\tvalid_0's rmse: 1.3362\n",
      "[1400]\tvalid_0's rmse: 1.33276\n",
      "[1500]\tvalid_0's rmse: 1.32905\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1500]\tvalid_0's rmse: 1.32905\n",
      "Training TX_1 ...\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's rmse: 1.62286\n",
      "[200]\tvalid_0's rmse: 1.60197\n",
      "[300]\tvalid_0's rmse: 1.58961\n",
      "[400]\tvalid_0's rmse: 1.57901\n",
      "[500]\tvalid_0's rmse: 1.56878\n",
      "[600]\tvalid_0's rmse: 1.5616\n",
      "[700]\tvalid_0's rmse: 1.55355\n",
      "[800]\tvalid_0's rmse: 1.54831\n",
      "[900]\tvalid_0's rmse: 1.54258\n",
      "[1000]\tvalid_0's rmse: 1.5366\n",
      "[1100]\tvalid_0's rmse: 1.53225\n",
      "[1200]\tvalid_0's rmse: 1.52795\n",
      "[1300]\tvalid_0's rmse: 1.52288\n",
      "[1400]\tvalid_0's rmse: 1.51908\n",
      "[1500]\tvalid_0's rmse: 1.51387\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1500]\tvalid_0's rmse: 1.51387\n",
      "Training TX_2 ...\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's rmse: 1.77566\n",
      "[200]\tvalid_0's rmse: 1.76282\n",
      "[300]\tvalid_0's rmse: 1.75376\n",
      "[400]\tvalid_0's rmse: 1.74481\n",
      "[500]\tvalid_0's rmse: 1.7367\n",
      "[600]\tvalid_0's rmse: 1.72963\n",
      "[700]\tvalid_0's rmse: 1.72408\n",
      "[800]\tvalid_0's rmse: 1.71914\n",
      "[900]\tvalid_0's rmse: 1.71297\n",
      "[1000]\tvalid_0's rmse: 1.70729\n",
      "[1100]\tvalid_0's rmse: 1.7023\n",
      "[1200]\tvalid_0's rmse: 1.69808\n",
      "[1300]\tvalid_0's rmse: 1.69265\n",
      "[1400]\tvalid_0's rmse: 1.68746\n",
      "[1500]\tvalid_0's rmse: 1.68353\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1496]\tvalid_0's rmse: 1.68343\n",
      "Training TX_3 ...\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's rmse: 1.85556\n",
      "[200]\tvalid_0's rmse: 1.83743\n",
      "[300]\tvalid_0's rmse: 1.82313\n",
      "[400]\tvalid_0's rmse: 1.81049\n",
      "[500]\tvalid_0's rmse: 1.7996\n",
      "[600]\tvalid_0's rmse: 1.78963\n",
      "[700]\tvalid_0's rmse: 1.77912\n",
      "[800]\tvalid_0's rmse: 1.77161\n",
      "[900]\tvalid_0's rmse: 1.76399\n",
      "[1000]\tvalid_0's rmse: 1.75648\n",
      "[1100]\tvalid_0's rmse: 1.7501\n",
      "[1200]\tvalid_0's rmse: 1.74377\n",
      "[1300]\tvalid_0's rmse: 1.73622\n",
      "[1400]\tvalid_0's rmse: 1.73065\n",
      "[1500]\tvalid_0's rmse: 1.72553\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1500]\tvalid_0's rmse: 1.72553\n",
      "Training WI_1 ...\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's rmse: 1.60244\n",
      "[200]\tvalid_0's rmse: 1.58803\n",
      "[300]\tvalid_0's rmse: 1.57796\n",
      "[400]\tvalid_0's rmse: 1.57024\n",
      "[500]\tvalid_0's rmse: 1.56298\n",
      "[600]\tvalid_0's rmse: 1.55634\n",
      "[700]\tvalid_0's rmse: 1.54982\n",
      "[800]\tvalid_0's rmse: 1.54423\n",
      "[900]\tvalid_0's rmse: 1.53929\n",
      "[1000]\tvalid_0's rmse: 1.53454\n",
      "[1100]\tvalid_0's rmse: 1.53024\n",
      "[1200]\tvalid_0's rmse: 1.52561\n",
      "[1300]\tvalid_0's rmse: 1.52163\n",
      "[1400]\tvalid_0's rmse: 1.51779\n",
      "[1500]\tvalid_0's rmse: 1.51371\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1498]\tvalid_0's rmse: 1.5137\n",
      "Training WI_2 ...\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's rmse: 2.61466\n",
      "[200]\tvalid_0's rmse: 2.57801\n",
      "[300]\tvalid_0's rmse: 2.55463\n",
      "[400]\tvalid_0's rmse: 2.53961\n",
      "[500]\tvalid_0's rmse: 2.5259\n",
      "[600]\tvalid_0's rmse: 2.50899\n",
      "[700]\tvalid_0's rmse: 2.49428\n",
      "[800]\tvalid_0's rmse: 2.48052\n",
      "[900]\tvalid_0's rmse: 2.47001\n",
      "[1000]\tvalid_0's rmse: 2.4572\n",
      "[1100]\tvalid_0's rmse: 2.44825\n",
      "[1200]\tvalid_0's rmse: 2.43939\n",
      "[1300]\tvalid_0's rmse: 2.42759\n",
      "[1400]\tvalid_0's rmse: 2.41751\n",
      "[1500]\tvalid_0's rmse: 2.40626\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1497]\tvalid_0's rmse: 2.4061\n",
      "Training WI_3 ...\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's rmse: 1.95156\n",
      "[200]\tvalid_0's rmse: 1.93452\n",
      "[300]\tvalid_0's rmse: 1.91807\n",
      "[400]\tvalid_0's rmse: 1.90381\n",
      "[500]\tvalid_0's rmse: 1.88892\n",
      "[600]\tvalid_0's rmse: 1.87871\n",
      "[700]\tvalid_0's rmse: 1.86675\n",
      "[800]\tvalid_0's rmse: 1.85788\n",
      "[900]\tvalid_0's rmse: 1.85332\n",
      "[1000]\tvalid_0's rmse: 1.84491\n",
      "[1100]\tvalid_0's rmse: 1.83551\n",
      "[1200]\tvalid_0's rmse: 1.82712\n",
      "[1300]\tvalid_0's rmse: 1.82105\n",
      "[1400]\tvalid_0's rmse: 1.81559\n",
      "[1500]\tvalid_0's rmse: 1.81\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1500]\tvalid_0's rmse: 1.81\n"
     ]
    }
   ],
   "source": [
    "for store_id in STORES_IDS:\n",
    "    print('Training '+store_id+' ...')\n",
    "    \n",
    "    # get the current store\n",
    "    df_combined = get_data_by_store(store_id)\n",
    "    all_columns = list(df_combined.columns)\n",
    "    features_columns = [col for col in all_columns if col not in remove_features]\n",
    "    \n",
    "    # prepare training, validating, and prediction dataset\n",
    "    train_ind = df_combined['d']<=END_TRAIN\n",
    "    valid_ind = train_ind&(df_combined['d']>(END_TRAIN-P_HORIZON))\n",
    "    preds_ind = df_combined['d']>(END_TRAIN-100)\n",
    "    \n",
    "    train_data = lgb.Dataset(df_combined[train_ind][features_columns], label=df_combined[train_ind][TARGET])\n",
    "    valid_data = lgb.Dataset(df_combined[valid_ind][features_columns], label=df_combined[valid_ind][TARGET])\n",
    "    \n",
    "    # save part of dataset for prediction\n",
    "    df_combined = df_combined[preds_ind].reset_index(drop=True)\n",
    "    # df_combined = df_combined[feature_columns]\n",
    "    df_combined.to_pickle(ModelFilePath+'test_'+store_id+'.pkl')\n",
    "    del df_combined\n",
    "    \n",
    "    # training\n",
    "    seed_everything(SEED)\n",
    "    estimator = lgb.train(lgb_params,\n",
    "                          train_data,\n",
    "                          valid_sets = [valid_data],\n",
    "                          verbose_eval = 100,\n",
    "                          )\n",
    "    \n",
    "    model_name = 'lgb_model_'+store_id+'_v'+str(VER)+'.bin'\n",
    "    pickle.dump(estimator, open(ModelFilePath+model_name, 'wb'))\n",
    "    \n",
    "    # free some ram\n",
    "    # !rm train_data.bin\n",
    "    del train_data, valid_data, estimator\n",
    "    gc.collect()\n",
    "    \n",
    "    MODEL_FEATURES = features_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2+2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions to make predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_test():\n",
    "    \"\"\"\n",
    "    The objective of this function is to read stored test dataset. The stored test dataset has 100 days before d1913 \n",
    "    in order to calculate lag and rolling\n",
    "    \"\"\"\n",
    "    base_test = pd.DataFrame()\n",
    "\n",
    "    for store_id in STORES_IDS:\n",
    "        temp_df = pd.read_pickle(ModelFilePath+'test_'+store_id+'.pkl')\n",
    "        temp_df['store_id'] = store_id\n",
    "        base_test = pd.concat([base_test, temp_df]).reset_index(drop=True)\n",
    "    \n",
    "    return base_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifts = [1, 7, 28]\n",
    "windows = [7, 28]\n",
    "lags = [7,28]\n",
    "lag_windows = [7,28]\n",
    "# rolling based on original sales\n",
    "def make_lag_roll(df):\n",
    "    \"\"\"\n",
    "    The function aims to make new lag and rolling for test data\n",
    "    \"\"\"\n",
    "    for shift in shifts:\n",
    "        df['lag'+str(shift)] = df.groupby('id')[TARGET].transform(lambda x: x.shift(shift))\n",
    "        \n",
    "    for window in windows:\n",
    "        df['rolling_mean'+str(window)] = df.groupby('id')[TARGET].transform(lambda x: x.rolling(window).mean()).astype(np.float16)\n",
    "        df['rolling_std'+str(window)] = df.groupby('id')[TARGET].transform(lambda x: x.rolling(window).std()).astype(np.float16)\n",
    "    for lag in lags:\n",
    "        for lag_window in lag_windows:\n",
    "            df['lag'+str(lag)+'rolling'+str(lag_window)]=df.groupby('id')[TARGET].transform(lambda x: x.shift(lag).rolling(lag_window).mean()).astype(np.float16) \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict | Day: 1\n",
      "##########  2.05 min round |  2.05 min total |  40135.17 day sales |\n",
      "Predict | Day: 2\n",
      "##########  2.05 min round |  4.11 min total |  38238.37 day sales |\n",
      "Predict | Day: 3\n",
      "##########  2.03 min round |  6.14 min total |  38784.35 day sales |\n",
      "Predict | Day: 4\n",
      "##########  2.02 min round |  8.16 min total |  37818.38 day sales |\n",
      "Predict | Day: 5\n",
      "##########  2.03 min round |  10.19 min total |  42831.77 day sales |\n",
      "Predict | Day: 6\n",
      "##########  2.00 min round |  12.19 min total |  50056.59 day sales |\n",
      "Predict | Day: 7\n",
      "##########  1.99 min round |  14.18 min total |  50739.56 day sales |\n",
      "Predict | Day: 8\n",
      "##########  1.99 min round |  16.17 min total |  46827.92 day sales |\n",
      "Predict | Day: 9\n",
      "##########  2.00 min round |  18.17 min total |  40191.24 day sales |\n",
      "Predict | Day: 10\n",
      "##########  1.96 min round |  20.14 min total |  44428.61 day sales |\n",
      "Predict | Day: 11\n",
      "##########  1.95 min round |  22.09 min total |  46219.84 day sales |\n",
      "Predict | Day: 12\n",
      "##########  1.96 min round |  24.04 min total |  54216.43 day sales |\n",
      "Predict | Day: 13\n",
      "##########  1.95 min round |  26.00 min total |  57283.90 day sales |\n",
      "Predict | Day: 14\n",
      "##########  1.95 min round |  27.95 min total |  60327.78 day sales |\n",
      "Predict | Day: 15\n",
      "##########  1.96 min round |  29.91 min total |  50968.63 day sales |\n",
      "Predict | Day: 16\n",
      "##########  1.99 min round |  31.89 min total |  45711.21 day sales |\n",
      "Predict | Day: 17\n",
      "##########  1.99 min round |  33.88 min total |  45056.19 day sales |\n",
      "Predict | Day: 18\n",
      "##########  1.99 min round |  35.87 min total |  47280.00 day sales |\n",
      "Predict | Day: 19\n",
      "##########  2.07 min round |  37.95 min total |  48662.20 day sales |\n",
      "Predict | Day: 20\n",
      "##########  2.04 min round |  39.99 min total |  59989.55 day sales |\n",
      "Predict | Day: 21\n",
      "##########  1.99 min round |  41.98 min total |  62663.76 day sales |\n",
      "Predict | Day: 22\n",
      "##########  1.99 min round |  43.97 min total |  47969.47 day sales |\n",
      "Predict | Day: 23\n",
      "##########  1.99 min round |  45.97 min total |  45726.89 day sales |\n",
      "Predict | Day: 24\n",
      "##########  1.99 min round |  47.96 min total |  47481.25 day sales |\n",
      "Predict | Day: 25\n",
      "##########  1.99 min round |  49.95 min total |  43159.86 day sales |\n",
      "Predict | Day: 26\n",
      "##########  1.99 min round |  51.94 min total |  47516.38 day sales |\n",
      "Predict | Day: 27\n",
      "##########  1.99 min round |  53.93 min total |  55496.39 day sales |\n",
      "Predict | Day: 28\n",
      "##########  1.99 min round |  55.92 min total |  51293.75 day sales |\n"
     ]
    }
   ],
   "source": [
    "all_preds = pd.DataFrame()\n",
    "base_test = get_base_test() # read data from all stores \n",
    "main_time = time.time()\n",
    "\n",
    "for PREDICT_DAY in range(1,29):    \n",
    "    print('Predict | Day:', PREDICT_DAY)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    df_grid = base_test.copy()\n",
    "    df_grid = make_lag_roll(df_grid)\n",
    "    \n",
    "    for store_id in STORES_IDS:\n",
    "        model_name = 'lgb_model_'+store_id+'_v'+str(VER)+'.bin' \n",
    "        estimator = pickle.load(open(ModelFilePath+model_name, 'rb'))\n",
    "        \n",
    "        # read test data for the store\n",
    "        day_ind = base_test['d']==(END_TRAIN+PREDICT_DAY)\n",
    "        store_ind = base_test['store_id']==store_id\n",
    "        total_ind = (day_ind)&(store_ind)\n",
    "        \n",
    "        base_test[TARGET][total_ind] = estimator.predict(df_grid[total_ind][MODEL_FEATURES])\n",
    "    \n",
    "    # create a sub-dataframe to store results\n",
    "    temp_df = base_test[day_ind][['id',TARGET]]\n",
    "    temp_df.columns = ['id','F'+str(PREDICT_DAY)]\n",
    "    \n",
    "    if 'id' in list(all_preds):\n",
    "        all_preds = all_preds.merge(temp_df, on=['id'], how='left') \n",
    "    else:\n",
    "        all_preds = temp_df.copy() # attach 'id' on the first day\n",
    "    \n",
    "    print('#'*10, ' {:0.2f} min round |'.format(((time.time() - start_time) / 60)),\n",
    "                  ' {:0.2f} min total |'.format(((time.time() - main_time) / 60)),\n",
    "                  ' {:0.2f} day sales |'.format((temp_df['F'+str(PREDICT_DAY)].sum())))\n",
    "    del temp_df\n",
    "        \n",
    "all_preds = all_preds.reset_index(drop=True)\n",
    "all_preds\n",
    "name = 'preds_all_ver'+ str(VER) +'.pkl'\n",
    "all_preds.to_pickle(PredictFilePath+name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
