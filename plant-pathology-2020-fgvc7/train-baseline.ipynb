{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = 'images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def make_folds(n_folds: int) -> pd.DataFrame:\n",
    "    df = pd.read_csv('train.csv')\n",
    "    cls_counts = 4\n",
    "    fold_cls_counts = defaultdict(int)\n",
    "    folds = [-1] * len(df)\n",
    "    exist_labels = [-1] * len(df)\n",
    "    for item in tqdm.tqdm(df.sample(frac=1, random_state=42).itertuples(),\n",
    "                          total=len(df)):\n",
    "        cls = item.attribute_ids\n",
    "        fold_counts = [(f, fold_cls_counts[f, cls]) for f in range(n_folds)]\n",
    "        min_count = min([count for _, count in fold_counts])\n",
    "        random.seed(item.Index)\n",
    "        fold = random.choice([f for f, count in fold_counts\n",
    "                              if count == min_count])\n",
    "        folds[item.Index] = fold\n",
    "        if item.attribute_ids != 0:\n",
    "           exist_labels[item.Index] = 1\n",
    "        else:\n",
    "           exist_labels[item.Index] = 0\n",
    "        #for cls in item.attribute_ids.split():\n",
    "        #    fold_cls_counts[fold, cls] += 1\n",
    "    df['fold'] = folds\n",
    "    df['exist_labels'] = exist_labels\n",
    "    return df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = make_folds(n_folds=5)\n",
    "df.to_csv('folds.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use ready-made already folded folds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = pd.read_csv('folds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>target</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Train_0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Train_1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Train_2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Train_3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Train_4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_id  target  fold\n",
       "0  Train_0       3     3\n",
       "1  Train_1       1     1\n",
       "2  Train_2       0     0\n",
       "3  Train_3       2     1\n",
       "4  Train_4       0     1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Callable, List\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, root: Path, df: pd.DataFrame,\n",
    "                 image_transform: Callable, debug: bool = True):\n",
    "        super().__init__()\n",
    "        self._root = root\n",
    "        self._df = df\n",
    "        self._image_transform = image_transform\n",
    "        self._debug = debug\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        item = self._df.iloc[idx]\n",
    "        image = load_transform_image(\n",
    "            item, self._root, self._image_transform, debug=self._debug)\n",
    "        target = torch.tensor(item.target)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class TTADataset:\n",
    "    def __init__(self, root: Path, df: pd.DataFrame,\n",
    "                 image_transform: Callable, tta: int):\n",
    "        self._root = root\n",
    "        self._df = df\n",
    "        self._image_transform = image_transform\n",
    "        self._tta = tta\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._df) * self._tta\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self._df.iloc[idx % len(self._df)]\n",
    "        image = load_transform_image(item, self._root, self._image_transform)\n",
    "        return image, item.image_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision.transforms import (\n",
    "    ToTensor, Normalize, Compose, Resize, CenterCrop, RandomCrop,\n",
    "    RandomHorizontalFlip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = Compose([\n",
    "    RandomCrop(288),\n",
    "    RandomHorizontalFlip(),\n",
    "])\n",
    "\n",
    "test_transform = Compose([\n",
    "    #RandomCrop(288),\n",
    "    RandomCrop(256),\n",
    "    RandomHorizontalFlip(),\n",
    "])\n",
    "\n",
    "tensor_transform = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.496,0.456,0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transform_image(\n",
    "        item, root: Path, image_transform: Callable, debug: bool = False):\n",
    "    image = load_image(item, root)\n",
    "    image = image_transform(image)\n",
    "    if debug:\n",
    "        image.save('_debug.png')\n",
    "    return tensor_transform(image)\n",
    "\n",
    "\n",
    "def load_image(item, root: Path) -> Image.Image:\n",
    "    #print(str(root + '/' + f'{item.image_id}.jpg'))\n",
    "    image = cv2.imread(str(root + '/' + f'{item.image_id}.jpg'))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return Image.fromarray(image)\n",
    "\n",
    "\n",
    "def get_ids(root: Path) -> List[str]:\n",
    "    return sorted({p.name.split('_')[0] for p in root.glob('*.jpg')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = pd.read_csv('folds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fold = folds[folds['fold'] != 0]\n",
    "valid_fold = folds[folds['fold'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "import json\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import warnings\n",
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import fbeta_score, roc_auc_score\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "import torch\n",
    "from torch import nn, cuda\n",
    "from torch.optim import Adam\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_root = DATA_ROOT #+ 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 4\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loader(df: pd.DataFrame, image_transform) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            TrainDataset(train_root, df, image_transform, debug=0),\n",
    "            shuffle=True,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=num_workers,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,434 items in train, 387 in valid\n"
     ]
    }
   ],
   "source": [
    "train_loader = make_loader(train_fold, train_transform)\n",
    "valid_loader = make_loader(valid_fold, test_transform)\n",
    "print(f'{len(train_loader.dataset):,} items in train, '\n",
    "      f'{len(valid_loader.dataset):,} in valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "import torchvision.models as M\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseCrossEntropy(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DenseCrossEntropy, self).__init__()\n",
    "        \n",
    "        \n",
    "    def forward(self, logits, labels):\n",
    "        logits = logits.float()\n",
    "        labels = labels.float()\n",
    "        \n",
    "        logprobs = F.log_softmax(logits, dim=-1)\n",
    "        \n",
    "        loss = -labels * logprobs\n",
    "        loss = loss.sum(-1)\n",
    "\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgPool(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return F.avg_pool2d(x, x.shape[2:])\n",
    "\n",
    "\n",
    "def create_net(net_cls, pretrained: bool):\n",
    "    if pretrained:\n",
    "        net = net_cls()\n",
    "        model_name = net_cls.__name__\n",
    "        weights_path = f'../input/{model_name}/{model_name}.pth'\n",
    "        net.load_state_dict(torch.load(weights_path))\n",
    "    else:\n",
    "        net = net_cls(pretrained=pretrained)\n",
    "    return net\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_classes,\n",
    "                 pretrained=False, net_cls=M.resnet50, dropout=False):\n",
    "        super().__init__()\n",
    "        self.net = create_net(net_cls, pretrained=pretrained)\n",
    "        self.net.avgpool = AvgPool()\n",
    "        if dropout:\n",
    "            self.net.fc = nn.Sequential(\n",
    "                nn.Dropout(),\n",
    "                nn.Linear(self.net.fc.in_features, num_classes),\n",
    "            )\n",
    "        else:\n",
    "            self.net.fc = nn.Linear(self.net.fc.in_features, num_classes)\n",
    "\n",
    "    def fresh_params(self):\n",
    "        return self.net.fc.parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, num_classes,\n",
    "                 pretrained=False, net_cls=M.densenet121):\n",
    "        super().__init__()\n",
    "        self.net = create_net(net_cls, pretrained=pretrained)\n",
    "        self.avg_pool = AvgPool()\n",
    "        self.net.classifier = nn.Linear(\n",
    "            self.net.classifier.in_features, num_classes)\n",
    "\n",
    "    def fresh_params(self):\n",
    "        return self.net.classifier.parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net.features(x)\n",
    "        out = F.relu(out, inplace=True)\n",
    "        out = self.avg_pool(out).view(out.size(0), -1)\n",
    "        out = self.net.classifier(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "resnet18 = partial(ResNet, net_cls=M.resnet18)\n",
    "resnet34 = partial(ResNet, net_cls=M.resnet34)\n",
    "resnet50 = partial(ResNet, net_cls=M.resnet50)\n",
    "resnet101 = partial(ResNet, net_cls=M.resnet101)\n",
    "resnet152 = partial(ResNet, net_cls=M.resnet152)\n",
    "\n",
    "densenet121 = partial(DenseNet, net_cls=M.densenet121)\n",
    "densenet169 = partial(DenseNet, net_cls=M.densenet169)\n",
    "densenet201 = partial(DenseNet, net_cls=M.densenet201)\n",
    "densenet161 = partial(DenseNet, net_cls=M.densenet161)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() #DenseCrossEntropy()#nn.BCEWithLogitsLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50(num_classes=N_CLASSES, pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fresh_params = list(model.fresh_params())\n",
    "all_params = list(model.parameters())\n",
    "if use_cuda:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    " train_kwargs = dict(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            train_loader=train_loader,\n",
    "            valid_loader=valid_loader,\n",
    "            patience=4,\n",
    "            init_optimizer=lambda params, lr: Adam(params, lr),\n",
    "            use_cuda=use_cuda,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model: nn.Module, path: Path) -> Dict:\n",
    "    state = torch.load(str(path))\n",
    "    model.load_state_dict(state['model'])\n",
    "    print('Loaded model from epoch {epoch}, step {step:,}'.format(**state))\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reduce_loss(loss):\n",
    "    print(loss)\n",
    "    return loss.sum() / loss.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_prediction(probabilities, threshold: float, argsorted=None,\n",
    "                        min_labels=1, max_labels=10):\n",
    "    \"\"\" Return matrix of 0/1 predictions, same shape as probabilities.\n",
    "    \"\"\"\n",
    "    assert probabilities.shape[1] == N_CLASSES\n",
    "    if argsorted is None:\n",
    "        argsorted = probabilities.argsort(axis=1)\n",
    "    max_mask = _make_mask(argsorted, max_labels)\n",
    "    min_mask = _make_mask(argsorted, min_labels)\n",
    "    prob_mask = probabilities > threshold\n",
    "    return (max_mask & prob_mask) | min_mask\n",
    "\n",
    "\n",
    "def _make_mask(argsorted, top_n: int):\n",
    "    mask = np.zeros_like(argsorted, dtype=np.uint8)\n",
    "    col_indices = argsorted[:, -top_n:].reshape(-1)\n",
    "    row_indices = [i // top_n for i in range(len(col_indices))]\n",
    "    mask[row_indices, col_indices] = 1\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(\n",
    "        model: nn.Module, criterion, valid_loader, use_cuda,\n",
    "        ) -> Dict[str, float]:\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_preds = None\n",
    "    val_labels = None\n",
    "    for inputs, targets in valid_loader:\n",
    "        images = inputs\n",
    "        labels = targets\n",
    "\n",
    "        if val_labels is None:\n",
    "            val_labels = labels.clone().squeeze(-1)\n",
    "        else:\n",
    "            val_labels = torch.cat((val_labels, labels.squeeze(-1)), dim=0)\n",
    "\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "\n",
    "            loss = criterion(outputs, labels.squeeze(-1))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            preds = torch.nn.functional.softmax(model(images), 1)\n",
    "            #print(preds)\n",
    "\n",
    "            if val_preds is None:\n",
    "                val_preds = preds\n",
    "            else:\n",
    "                val_preds = torch.cat((val_preds, preds), dim=0)\n",
    "            \n",
    "    metrics = {}\n",
    "    metrics['valid_loss'] = np.mean(val_loss)\n",
    "    metrics['roc_auc'] = roc_auc_score(val_labels, val_preds.cpu(), multi_class=\"ovr\",average='weighted')\n",
    "    print(' | '.join(f'{k} {v:.3f}' for k, v in sorted(metrics.items(), key=lambda kv: -kv[1])))\n",
    "    return metrics\n",
    "#     all_losses, all_predictions, all_targets = [], [], []\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, targets in valid_loader:\n",
    "#             all_targets.append(targets.numpy().copy())\n",
    "#             if use_cuda:\n",
    "#                 inputs, targets = inputs.cuda(), targets.cuda()\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, targets)\n",
    "#             #all_losses.append(_reduce_loss(loss).item())\n",
    "#             all_losses.append(loss)\n",
    "#             #print(outputs)\n",
    "#             predictions = torch.softmax(outputs, dim=1).data.cpu()#torch.argmax(outputs)\n",
    "#             all_predictions.append(predictions.cpu().numpy())\n",
    "#     all_predictions = np.concatenate(all_predictions)\n",
    "#     all_targets = np.concatenate(all_targets)\n",
    "\n",
    "#     def get_score(y_pred):\n",
    "#         print(all_targets)\n",
    "#         with warnings.catch_warnings():\n",
    "#             warnings.simplefilter('ignore', category=UndefinedMetricWarning)\n",
    "#             return roc_auc_score(\n",
    "#                 all_targets, y_pred)#, beta=2, average='samples')\n",
    "\n",
    "#     metrics = {}\n",
    "#     argsorted = all_predictions.argsort(axis=1)\n",
    "# #     for threshold in [0.10, 0.20]:\n",
    "# #         metrics[f'valid_f2_th_{threshold:.2f}'] = get_score(\n",
    "# #             binarize_prediction(all_predictions, threshold, argsorted))\n",
    "# #     metrics['valid_loss'] = np.mean(all_losses)\n",
    "#     print(all_predictions)\n",
    "#     get_score(all_predictions)\n",
    "#     print(' | '.join(f'{k} {v:.3f}' for k, v in sorted(\n",
    "#         metrics.items(), key=lambda kv: -kv[1])))\n",
    "\n",
    "#     return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train( model: nn.Module, criterion, *, params,\n",
    "          train_loader, valid_loader, init_optimizer, use_cuda,\n",
    "          n_epochs=None, patience=2, max_lr_changes=2) -> bool:\n",
    "    \n",
    "    lr = 1e-4\n",
    "    batch_size = 32\n",
    "    n_epochs = 40\n",
    "    params = list(params)\n",
    "    optimizer = init_optimizer(params, lr)\n",
    "\n",
    "    model_path = 'model.pt'\n",
    "    best_model_path = 'best-model.pt'\n",
    "    uptrain = False\n",
    "    if uptrain:\n",
    "        state = load_model(model, model_path)\n",
    "        epoch = state['epoch']\n",
    "        step = state['step']\n",
    "        best_valid_loss = state['best_valid_loss']\n",
    "    else:\n",
    "        epoch = 1\n",
    "        step = 0\n",
    "        best_valid_loss = float('inf')\n",
    "    lr_changes = 0\n",
    "\n",
    "    save = lambda ep: torch.save({\n",
    "        'model': model.state_dict(),\n",
    "        'epoch': ep,\n",
    "        'step': step,\n",
    "        'best_valid_loss': best_valid_loss\n",
    "    }, str(model_path))\n",
    "\n",
    "    report_each = 100\n",
    "    valid_losses = []\n",
    "    lr_reset_epoch = epoch\n",
    "    for epoch in range(epoch, n_epochs + 1):\n",
    "        model.train()\n",
    "        tq = tqdm.tqdm(total=(len(train_loader) * batch_size))\n",
    "        tq.set_description(f'Epoch {epoch}, lr {lr}')\n",
    "        losses = []\n",
    "        tl = train_loader\n",
    "        try:\n",
    "            mean_loss = 0\n",
    "            for i, (inputs, targets) in enumerate(tl):\n",
    "                if use_cuda:\n",
    "                    inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                outputs = model(inputs)\n",
    "                #print(outputs, targets)\n",
    "                #loss = _reduce_loss(criterion(outputs, targets))\n",
    "                loss = criterion(outputs, targets)\n",
    "                batch_size = inputs.size(0)\n",
    "                #(batch_size * loss).backward()\n",
    "                loss.backward()\n",
    "                if (i + 1) % 1 == 0:\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    step += 1\n",
    "                tq.update(batch_size)\n",
    "                #print(loss.item())\n",
    "                #losses.append(loss.item())\n",
    "                losses.append(loss)\n",
    "                #print(len(losses))\n",
    "                mean_loss = losses[0]# np.mean(losses[-report_each:])\n",
    "                tq.set_postfix(loss=f'{mean_loss:.3f}')\n",
    "            tq.close()\n",
    "            save(epoch + 1)\n",
    "            valid_metrics = validation(model, criterion, valid_loader, use_cuda)\n",
    "            \n",
    "            valid_loss = valid_metrics['valid_loss']\n",
    "            valid_losses.append(valid_loss)\n",
    "            if valid_loss < best_valid_loss:\n",
    "                best_valid_loss = valid_loss\n",
    "                shutil.copy(str(model_path), str(best_model_path))\n",
    "            elif (patience and epoch - lr_reset_epoch > patience and\n",
    "                  min(valid_losses[-patience:]) > best_valid_loss):\n",
    "                lr_changes +=1\n",
    "                if lr_changes > max_lr_changes:\n",
    "                    break\n",
    "                lr /= 5\n",
    "                print(f'lr updated to {lr}')\n",
    "                lr_reset_epoch = epoch\n",
    "                optimizer = init_optimizer(params, lr)\n",
    "        except KeyboardInterrupt:\n",
    "            tq.close()\n",
    "            print('Ctrl+C, saving snapshot')\n",
    "            save(epoch)\n",
    "            print('done.')\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, lr 0.0001: : 1434it [00:12, 116.27it/s, loss=1.650]\n",
      "Epoch 2, lr 0.0001:   0%|          | 0/598 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss 9.027 | roc_auc 0.568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, lr 0.0001: : 1434it [00:12, 111.78it/s, loss=1.289]\n",
      "Epoch 3, lr 0.0001:   0%|          | 0/598 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss 9.453 | roc_auc 0.584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, lr 0.0001: : 1434it [00:12, 112.51it/s, loss=1.176]\n",
      "Epoch 4, lr 0.0001:   0%|          | 0/598 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss 9.141 | roc_auc 0.633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, lr 0.0001: : 1434it [00:13, 109.33it/s, loss=1.280]\n",
      "Epoch 5, lr 0.0001:   0%|          | 0/598 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss 11.983 | roc_auc 0.600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, lr 0.0001: : 1434it [00:12, 111.95it/s, loss=1.125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss 8.741 | roc_auc 0.627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, lr 0.0001: : 1434it [00:12, 110.41it/s, loss=1.203]\n",
      "Epoch 7, lr 0.0001:   0%|          | 0/598 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss 10.379 | roc_auc 0.635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, lr 0.0001: : 1434it [00:12, 112.46it/s, loss=1.177]\n",
      "Epoch 8, lr 0.0001:   0%|          | 0/598 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss 10.306 | roc_auc 0.610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, lr 0.0001: : 1434it [00:12, 112.08it/s, loss=1.104]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss 8.421 | roc_auc 0.646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, lr 0.0001: : 1434it [00:12, 112.27it/s, loss=1.157]\n",
      "Epoch 10, lr 0.0001:   0%|          | 0/598 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss 8.729 | roc_auc 0.653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, lr 0.0001: : 1434it [00:12, 111.16it/s, loss=1.138]\n",
      "Epoch 11, lr 0.0001:   0%|          | 0/598 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss 9.023 | roc_auc 0.695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, lr 0.0001: : 1434it [00:12, 115.75it/s, loss=1.157]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss 8.399 | roc_auc 0.687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, lr 0.0001: : 1434it [00:12, 112.09it/s, loss=1.060]\n",
      "Epoch 13, lr 0.0001:   0%|          | 0/598 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss 9.344 | roc_auc 0.615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, lr 0.0001: : 1434it [00:12, 112.64it/s, loss=1.117]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss 8.083 | roc_auc 0.681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, lr 0.0001: : 1434it [00:12, 111.31it/s, loss=1.202]\n",
      "Epoch 15, lr 0.0001:   0%|          | 0/598 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss 8.153 | roc_auc 0.683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, lr 0.0001: : 1434it [00:12, 112.28it/s, loss=1.126]\n",
      "Epoch 16, lr 0.0001:   0%|          | 0/598 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss 8.175 | roc_auc 0.704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, lr 0.0001: : 1434it [00:13, 107.68it/s, loss=1.094]\n",
      "Epoch 17, lr 0.0001:   0%|          | 0/598 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss 8.575 | roc_auc 0.631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, lr 0.0001: : 1434it [00:13, 109.67it/s, loss=0.966]\n",
      "Epoch 18, lr 2e-05:   0%|          | 0/598 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss 9.074 | roc_auc 0.664\n",
      "lr updated to 2e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, lr 2e-05: : 1434it [00:13, 109.59it/s, loss=1.142]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss 7.707 | roc_auc 0.713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, lr 2e-05: : 1434it [00:12, 112.52it/s, loss=1.055]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss 7.610 | roc_auc 0.677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20, lr 2e-05: : 1434it [00:12, 111.64it/s, loss=1.080]\n",
      "Epoch 21, lr 2e-05:   0%|          | 0/598 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss 8.824 | roc_auc 0.685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21, lr 2e-05: : 1434it [00:12, 110.54it/s, loss=1.177]\n",
      "Epoch 22, lr 2e-05:   0%|          | 0/598 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss 8.485 | roc_auc 0.694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22, lr 2e-05: : 1434it [00:13, 107.80it/s, loss=1.153]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss 7.564 | roc_auc 0.693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23, lr 2e-05: : 1434it [00:12, 112.38it/s, loss=0.998]\n",
      "Epoch 24, lr 2e-05:   0%|          | 0/598 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss 8.429 | roc_auc 0.688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24, lr 2e-05: : 1434it [00:12, 111.33it/s, loss=0.949]\n",
      "Epoch 25, lr 2e-05:   0%|          | 0/598 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss 7.937 | roc_auc 0.681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25, lr 2e-05: : 1434it [00:12, 111.75it/s, loss=1.264]\n",
      "Epoch 26, lr 2e-05:   0%|          | 0/598 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss 8.431 | roc_auc 0.695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26, lr 2e-05: : 1434it [00:12, 112.52it/s, loss=1.004]\n",
      "Epoch 27, lr 4.000000000000001e-06:   0%|          | 0/598 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss 8.513 | roc_auc 0.725\n",
      "lr updated to 4.000000000000001e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27, lr 4.000000000000001e-06: : 1434it [00:12, 111.54it/s, loss=1.211]\n",
      "Epoch 28, lr 4.000000000000001e-06:   0%|          | 0/598 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss 8.438 | roc_auc 0.711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28, lr 4.000000000000001e-06: : 1434it [00:12, 112.49it/s, loss=1.097]\n",
      "Epoch 29, lr 4.000000000000001e-06:   0%|          | 0/598 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss 7.648 | roc_auc 0.716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29, lr 4.000000000000001e-06: : 1434it [00:13, 107.87it/s, loss=1.002]\n",
      "Epoch 30, lr 4.000000000000001e-06:   0%|          | 0/598 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss 8.304 | roc_auc 0.730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30, lr 4.000000000000001e-06: : 1434it [00:12, 110.89it/s, loss=1.097]\n",
      "Epoch 31, lr 4.000000000000001e-06:   0%|          | 0/598 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss 8.116 | roc_auc 0.704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31, lr 4.000000000000001e-06: : 1434it [00:13, 110.21it/s, loss=0.993]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_loss 7.615 | roc_auc 0.717\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(params=all_params, **train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
